# ERA_Assignments
Repository of case studies from the Extensive &amp; Reimagined AI Program by The School of AI

| Case Study | Description |
|------------|-------------|
| [Multiclass Classification - 01](https://github.com/bala1802/ERA/tree/main/Session-9) | Constructed a Neural Network for the CIFAR10 dataset Multiclass classification, based on specific constraints |
| [Neural Network](https://github.com/bala1802/ERA/tree/main/Session-5) | Constructed a Neural Network for the MNIST dataset using PyTorch |
| [Backpropogation](https://github.com/bala1802/ERA/tree/main/Session-6/Part-1) | An overview of the backpropagation algorithm in Neural Networks, explaining its purpose and how it works |
| [Multiclass Classification - 02](https://github.com/bala1802/ERA/tree/main/Session-6/Part-2) | To achieve 99.4% validation accuracy on the MNIST Dataset. |
| [Multiclass Classification - 03](https://github.com/bala1802/ERA/tree/main/Session-7) | To achieve 99.4% validation accuracy on the MNIST Dataset with a Neural Network containing less than 8K parameters . |
| [Multiclass Classification - 04](https://github.com/bala1802/ERA/tree/main/Session-8) | To achieve 70% Test accuracy for the CIFAR-10 dataset  . |
| [Multiclass Classification - 05](https://github.com/bala1802/ERA/tree/main/Session-9) | The Neural Network architecture with multiple blocks and each block containing 3 Convolutional Layers, and multiple other constaints  . |
| [PyTorch Lightning Module](https://github.com/bala1802/lightning_module) | A lightning module to handle the PyTorch Lightning libraries  . |
| [Application deployment in HuggingFace Spaces](https://github.com/bala1802/ERA-Session-12) | A step-by-step process to deploy the Neural Network - PyTorch based application in the HuggingFace Spaces  . |
| [Variational Auto Encoder](https://github.com/bala1802/Variational_Auto_Encoder) | Implemented the Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling paper from the scratch using PyTorch to get a good understanding on the concepts used in Variational Auto Encoder  . |
| [Transformers Architecture Part-01](https://github.com/bala1802/ERA-Session-15) | Implemented English to Italian translator using the Transformers Architecture  . |
| [Transformers Architecture Part-2](https://github.com/bala1802/Neural-Networks-and-Deep-Learning/blob/master/Transformers/Experiment01/README.md) | Implemented the Transformers Arcchitecture from the Scratch. The purpose of this case-study is to understand what happens internally in the Transformers code. The Input and Output are passed to the Encoder, Decoder blocks to understand how the data is handled, what will be the output and shape of the tensors in each stage. |
| [Transformers Architecture Part-3](https://github.com/bala1802/Neural-Networks-and-Deep-Learning/blob/master/Transformers/Experiment02/Understanding%20Input%20Embedding%20Layer.ipynb) | Purpose is to understand the Input Embeddings in the Transformer's architecture as explained in the Attention Is All You Need paper |
| [Transformers Architecture Part-4](https://github.com/bala1802/Neural-Networks-and-Deep-Learning/blob/master/Transformers/Experiment02/Understanding%20Positional%20Encoding%20Layer.ipynb) | Purpose is to understand the Positional Encoding, Sinusoidal Wave, Wave lengths in Geometric Progression, Linear Relationships for Relative Positions from the paper "Attention Is All You Need".  |
| [UNET Architecture Part-1](https://github.com/bala1802/UNET) | Implementation of the U-Net Architecture using PyTorch. This implementation is in reference to the paper U-Net: Convolutional Networks for Biomedical Image Segmentation   |
| [UNET Architecture Part-2](https://github.com/bala1802/UNET_OxfordPetData) | Trained a UNET model on the Oxford Pet Dataset (`torchvision.datasets.OxfordIIITPet`) |
| [OpenAI CLIP](https://github.com/bala1802/OpenAI_CLIP/tree/main) | Implemented Open AI's CLIP Paper: Learning Transferable Visual Models From Natural Language Supervision from scratch using PyTorch   |
| [Fine-tuning Microsoft Phi-2 Small Language Model](https://github.com/bala1802/Phi2) | Fine-tuning of a Microsoft's Phi-2 small language model, aiming to enhance its capabilities and adapt it to specific tasks or domains.   |
| [Low Rank Adaptation (LoRA)](https://github.com/bala1802/LoRA) | PyTorch implementation demonstrating Low Rank Adaption (LoRA) in Neural Networks, for efficient model compression and fine-tuning on domain specific tasks   |
| [Rotary Positional Embeddings (RoPE)](https://github.com/bala1802/RoPE) | Exploring the concept of Rotary Positional Embeddings, and how they integrate the strengths of both Absolute Positional Embeddings and Relative Positional Embeddings.   |
